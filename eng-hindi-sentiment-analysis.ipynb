{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3483254,"sourceType":"datasetVersion","datasetId":2096389},{"sourceId":3530000,"sourceType":"datasetVersion","datasetId":2123426},{"sourceId":6339512,"sourceType":"datasetVersion","datasetId":3649794}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras.datasets import imdb\nimport pandas as pd\nimport numpy as np\nfrom keras.layers import LSTM, Activation, Dropout, Dense, Input, Conv1D, MaxPooling1D, GlobalMaxPooling1D\nfrom keras.layers import Embedding\nfrom keras.models import Model\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport keras\nfrom sklearn.model_selection import train_test_split\nfrom keras import Sequential","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk import word_tokenize, pos_tag\nimport os, re, csv, math, codecs\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport torch\nimport torch.nn as nn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nimport re\nfrom spacy.lang.hi import STOP_WORDS as STOP_WORDS_HI","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hindi text\nwith codecs.open(\"../input/sentiment-analysis/data/emoji_prediction_data/hindi_data/hindi.txt\", encoding='utf-8') as f:\n    hindi_text = f.read()#to read the total file\nprint(hindi_text[0:1000])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_hindi = pd.read_csv('../input/sentiment-analysis/data/emoji_prediction_data/hindi_data/hindi.csv', header=None)\nsentiment_hindi = sentiment_hindi[0].tolist() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#HINDI\n#postive=2, neutral=0, negative=1\nfor i in range(0,len(sentiment_hindi)):\n    if(sentiment_hindi[i]==19 or sentiment_hindi[i]==18 or sentiment_hindi[i]==17 or sentiment_hindi[i]==13 or sentiment_hindi[i]==12 or sentiment_hindi[i]==11 or sentiment_hindi[i]==9 or sentiment_hindi[i]==8):\n        sentiment_hindi[i]=\"neutral\"\n    elif(sentiment_hindi[i]==15 or sentiment_hindi[i]==16):\n        sentiment_hindi[i]=\"negative\"\n    else:\n        sentiment_hindi[i]=\"positive\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hindi\ndef remove_punct_hi(text):\n    new_words = \" \"\n    for word in text:\n        if word not in STOP_WORDS_HI:\n            w = re.sub(r'[.#-:?!\\_|\"\"'',-...A-Za-z0-9]','',word) #remove everything except words and space#how \n            new_words = new_words + w\n    return re.sub(r\"\\s+\", \" \", new_words, flags=re.UNICODE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hindi_text = hindi_text.splitlines()\n#print(remove_punct(text_cleaned[0:10000]))\nfiltered_sentence_hi = []\nfor i in range(0,len(hindi_text)):\n    filtered_sentence_hi.append(remove_punct_hi(hindi_text[i]))\nprint(filtered_sentence_hi[0:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing LabelEncoder from Sklearn \n# library from preprocessing Module.\nfrom sklearn.preprocessing import LabelEncoder\n \n# Creating a instance of label Encoder.\nle = LabelEncoder()\n \n# Using .fit_transform function to fit label\n# encoder and return encoded label\nlabel = le.fit_transform(sentiment_hindi[0:len(filtered_sentence_hi)])\n\nlabel_inverse = le.inverse_transform(label)\nset(label_inverse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=filtered_sentence_hi\nY_train=label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'Hindi_text':X_train,'Sentiment':Y_train})\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Sentiment'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# n = 40000\n# msk = df.groupby('Sentiment')['Sentiment'].transform('size') >= n\n# df = pd.concat((df[msk].groupby('Sentiment').sample(n=n), df[~msk]), ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Sentiment'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install googletrans==3.1.0a0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from googletrans import Translator\n\n# Initialize the translator\ntranslator = Translator()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Sentiment'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" df['English_text'] = df['Hindi_text'].apply(lambda text: translator.translate(text, src='hi', dest='en').text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop([\"Sentiment\"], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test,Y_train, Y_test = train_test_split(X, df['Sentiment'], stratify=df['Sentiment'], test_size=0.25, random_state = 45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = list(df['Hindi_text'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hindi_tokenizer = Tokenizer()\n\nall_sentences = X_train + X_test\n\nhindi_tokenizer.fit_on_texts(X['Hindi_text'].values)\n\n# + 1 for unknown token\nvocabSize = len(hindi_tokenizer.word_index) +1\n\nX_train_seq = hindi_tokenizer.texts_to_sequences(X_train['Hindi_text'])\nX_test_seq = hindi_tokenizer.texts_to_sequences(X_test['Hindi_text'])\n# X_val_seq = hindi_tokenizer.texts_to_sequences(X_val['Text'])\n# Initializing max length of sentence to 20 words\nmax_length = 20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_seq_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\nX_test_seq_pad = pad_sequences(X_test_seq, maxlen=max_length,padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/embedding/indicnlp.v1.hi.vec.gz\n!gzip -d /kaggle/working/indicnlp.v1.hi.vec.gz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabSize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read GloVE embeddings\npath_to_glove_file = '/kaggle/working/indicnlp.v1.hi.vec'\nnum_tokens = vocabSize\nembedding_dim = 300 #latent factors or features  \nhits = 0\nmisses = 0\nembeddings_index = {}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load GloVe word embeddings from the specified file\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\nprint(\"Found %s word vectors.\" % len(embeddings_index))\n\n# Initialize an embedding matrix for our neural network\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\n\n# Assign pre-trained word vectors to our vocabulary\nfor word, i in hindi_tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words found in the embedding index are assigned their respective vectors\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        # Words not found in the embedding index are set to all-zeros\n        # This includes the representation for \"padding\" and \"OOV\" (Out of Vocabulary)\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_dims = 256\n\nmodel = Sequential()\nmodel.add(Embedding(vocabSize, 300, input_length=max_length, weights=[embedding_matrix],trainable=False))\nmodel.add(LSTM(units = 256,return_sequences=True))\nmodel.add(LSTM(units = 128, dropout = 0.5,recurrent_dropout = 0.5,return_sequences=True))\nmodel.add(LSTM(units = 64, dropout = 0.1,recurrent_dropout = 0.1,return_sequences=True))\nmodel.add(LSTM(units = 16))\nmodel.add(Dense(3, activation='softmax'))\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train_seq_pad, Y_train, epochs = 20, batch_size = 256, validation_data=(X_test_seq_pad, Y_test), shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" df_train['Text_en'] = df_train['Text'].apply(lambda text: translator.translate(text, src='hi', dest='en').text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train[df_train['Text'] != df_train['Text_en']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['label'] = label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}